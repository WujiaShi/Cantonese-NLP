{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Detection in Cantonese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RovMOLsIS1Ha",
        "colab_type": "text"
      },
      "source": [
        "##Build Cantonese emotion lexicon\n",
        "\n",
        "The cantonoese sentiment lexicon is built based on a Cantonese-English Dictionary. Then IBM Watson tone analyser is used to compute tone and sentiment score for words. The service can return results for the following tone IDs: anger, fear, joy, and sadness (emotional tones); analytical, confident, and tentative (language tones). The service returns results only for tones whose scores meet a minimum threshold of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LDARfZnXSuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade ibm-watson"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGfKlf65V0-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jieba\n",
        "from pprint import pprint\n",
        "from ibm_watson import ToneAnalyzerV3\n",
        "import json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qqJdyBbStJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1Wy75DNIEOx8rpYBVQVU-TP4nDTnlC_16'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('cccanto-webdist.txt')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xywKaLU_TcIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the original canto-eng dict in\n",
        "with open(\"cccanto-webdist.txt\", encoding=\"utf8\") as f:\n",
        "    content = [line.split(\"/\") for line in f]\n",
        "\n",
        "#remove irrelenvent information   \n",
        "words_list = content[13:]\n",
        "\n",
        "def remove_romanization(line):\n",
        "    words = line.split()\n",
        "    first_word = words[0]\n",
        "    return first_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFtZ1PipTl6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calling api from IBM cloud\n",
        "tone_analyzer = ToneAnalyzerV3(\n",
        "    version='2017-09-21',\n",
        "    iam_apikey='AVWex58O1RxByFwP6vX1kAbK9tNNFdp_yQMOq7FEpsPo',\n",
        "    url='https://gateway.watsonplatform.net/tone-analyzer/api'\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9hNCRN1UVf7",
        "colab_type": "text"
      },
      "source": [
        "### Build emotion lexicon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ9Nf1jSTqoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tone(lst):\n",
        "    sentence_list = []\n",
        "    for sentence in lst:\n",
        "        sentence_list = ','.join(lst)\n",
        "    tone_analysis = tone_analyzer.tone(\n",
        "    {'text': sentence_list},\n",
        "    content_type='application/json').get_result()\n",
        "    docu_tone = tone_analysis['document_tone']\n",
        "    return docu_tone\n",
        "  \n",
        "emo_dict ={}\n",
        "for lst in words_list:\n",
        "    definition_list =[]\n",
        "    try:\n",
        "        for idx, elem in enumerate(lst):\n",
        "            if len(elem) > 2:\n",
        "                if idx != 0:\n",
        "                    definition_list.append(elem)\n",
        "                else:\n",
        "                    #remove the remanization of the words\n",
        "                    word_meaning = remove_romanization(elem)\n",
        "                    #add the word as key into dict\n",
        "                    emo_dict[word_meaning] = None\n",
        "        #calculate the sentiment score for the word\n",
        "        sscore = get_tone(definition_list)\n",
        "        full_word = lst[0]\n",
        "        word_key = remove_romanization(full_word)\n",
        "        #update the sentiment score for the word in dict\n",
        "        emo_dict[word_key] = sscore\n",
        "    except:\n",
        "        pass "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MZqxGtZUiWe",
        "colab_type": "text"
      },
      "source": [
        "## Analyse emotional tones in Cantonese posts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ciTliJcUiHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1hBPhrxATm88P7glLNitOvPKdv7K7tNcz'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(\"HKSocialMedia.csv\")\n",
        "\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1x_CcPBL8iowGWq7NpG7jrnEXoGKUdMD9'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('stopwords.txt') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7duqKLXeUslS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"HKSocialMedia.csv\")\n",
        "posts = data['post_text']\n",
        "print(posts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvjwGKFFWaNx",
        "colab_type": "text"
      },
      "source": [
        "### pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaED89JJVZoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove punctuations\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '《', '》', '【', '】','▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '「','」','»', '！', '。','，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def remove_punctuation(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, '')\n",
        "    return x\n",
        "\n",
        "texts = []\n",
        "for line in posts:\n",
        "    sentence = remove_punctuation(line)\n",
        "    texts.append(sentence)\n",
        "\n",
        "#remove stopwords\n",
        "import codecs\n",
        "\n",
        "def seg_word(sentence):\n",
        "    #segmentation\n",
        "    seg_list = jieba.cut(sentence)\n",
        "    seg_result = []\n",
        "    for w in seg_list:\n",
        "        seg_result.append(w)\n",
        "    #read stop_words\n",
        "    stopwords = set()\n",
        "    fr = codecs.open('stopwords.txt', 'r', 'utf-8')\n",
        "    for word in fr:\n",
        "        stopwords.add(word.strip())\n",
        "    fr.close()\n",
        "    return list(filter(lambda x: x not in stopwords, seg_result)) \n",
        "  \n",
        "seg_res = []\n",
        "for line in texts:\n",
        "    new_line = seg_word(line)\n",
        "    seg_res.append(new_line)\n",
        "pprint(seg_res[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_bqos1RViUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sen_score(lst):\n",
        "    score_list = []\n",
        "    avg_dict = {}\n",
        "    for word in lst:\n",
        "        if word in emo_dict:\n",
        "            score = emo_dict[word]\n",
        "            score_list.append(score)\n",
        "    for score_dict in score_list:\n",
        "        for key in score_dict:\n",
        "             if key not in avg_dict:\n",
        "                 avg_dict[key] = score_dict[key]\n",
        "              else:\n",
        "                 avg_dict[key] = avg_dict[key] + score_dict[key]\n",
        "    for key in avg_dict:\n",
        "        avg_dict[key] = avg_dict[key]/len(score_list)\n",
        "    return avg_dict\n",
        "  \n",
        "posts_sen_score = {}       \n",
        "for idx, post in enumerate(seg_res):\n",
        "    score = get_sen_score(post)\n",
        "    full_post = posts[idx]\n",
        "    posts_sen_score[full_post] = score\n",
        "    \n",
        "dict(list(posts_sen_score.items())[0:20])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}